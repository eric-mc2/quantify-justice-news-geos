{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from spacy.util import load_config\n",
    "from spacy import Language\n",
    "\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "import optuna\n",
    "\n",
    "from scripts.entity_recognition.components import block_matcher, intersection_matcher, street_vs_neighborhood\n",
    "import scripts.entity_recognition.operations as ops\n",
    "from scripts.utils.config import Config\n",
    "from scripts.utils import flatten_config\n",
    "from scripts.utils.spacy import load_spacy, load_metrics\n",
    "from scripts.utils.optuna import ArchiveBestModelCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"entity_recognition_models\"\n",
    "task = \"entity_recognition\"\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"Prototype model architectures for place / person recognition.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"qjn\",\n",
    "    \"task\": task,\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "hyperparams = set([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8081\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.set_experiment_tags(experiment_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_log_eval(run_name, params, model_path, nested=False):\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = load_metrics(model_path, 'ner')\n",
    "    best_model_path = os.path.join(model_path, \"model-best\")\n",
    "\n",
    "    # Load model params\n",
    "    repo = Repo(config._LOCAL_PROJECT_DIR, search_parent_directories=True)\n",
    "    params['git_hash'] = repo.heads.main.commit.hexsha\n",
    "\n",
    "    # Reshape params for logging\n",
    "    params = flatten_config(params)\n",
    "    # XXX: This migth have a bug converting children to strings?\n",
    "    params = {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}\n",
    "\n",
    "    with mlflow.start_run(run_name=run_name, nested=nested) as run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.spacy.log_model(load_spacy(best_model_path), run_name)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "train_path = config.get_data_path(f\"{task}.labels_train\")\n",
    "dev_path = config.get_data_path(f\"{task}.labels_dev\")\n",
    "base_cfg = config.get_file_path(f\"{task}.base_cfg\")\n",
    "full_cfg = config.get_file_path(f\"{task}.full_cfg\")\n",
    "out_path = config.get_file_path(f\"{task}.trained_model\")\n",
    "out_path_scratch = config.get_file_path(f\"{task}.trained_model\", scratch=True)\n",
    "comm_area_path = config.get_data_path(\"geoms.comm_areas\")\n",
    "neighborhood_path = config.get_data_path(\"geoms.neighborhoods\")\n",
    "street_name_path = config.get_data_path(\"geoms.street_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Language.component('block_matcher', func=block_matcher)\n",
    "Language.component('intersection_matcher', func=intersection_matcher)\n",
    "Language.component('street_vs_neighborhood', func=street_vs_neighborhood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(load_config(full_cfg).interpolate())\n",
    "mlflow_log_eval(\"quickstart_model\", params, out_path);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_base(trial, overrides = {}):\n",
    "    print(\"Training with overrides:\\n\", overrides)\n",
    "    ops.train(base_cfg, full_cfg, train_path, dev_path, out_path_scratch, comm_area_path, neighborhood_path, street_name_path, overrides)\n",
    "    \n",
    "    # Train will keep base config and apply overrides at run-time.\n",
    "    # So we load the config with the overrides for logging.\n",
    "    params = dict(load_config(full_cfg, overrides).interpolate())\n",
    "\n",
    "    run_name = f\"optuna_trial_{trial.number}\"\n",
    "    metrics = mlflow_log_eval(run_name, params, out_path_scratch, nested=True)\n",
    "\n",
    "    return metrics['ents_f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=experiment_tags['task'],\n",
    "                            direction=\"maximize\",\n",
    "                            storage=config.get_param(f\"{task}.optuna_db\"),\n",
    "                            load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archiver = ArchiveBestModelCallback(out_path=out_path, out_path_scratch=out_path_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best(): \n",
    "    best = mlflow.search_runs(\n",
    "        experiment_names=[experiment_name],\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=1,\n",
    "        order_by=[\"metrics.ents_f DESC\"],\n",
    "    ).iloc[0].T\n",
    "    return best\n",
    "\n",
    "def best_metrics():\n",
    "    best = get_best()\n",
    "    return best[best.index.str.startswith(\"metrics\")].to_dict()\n",
    "\n",
    "def best_params(keys):\n",
    "    best = get_best()\n",
    "    return {key: best.loc[\"params.\" + key] for key in keys}\n",
    "    \n",
    "def best_model():\n",
    "    best = get_best()\n",
    "    return os.path.join(best['artifact_uri'], best['tags.mlflow.runName'], 'model.spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = [\"paths.vectors\", \"components.ner.source\", \"components.tok2vec.source\", \"initialize.vectors\"]\n",
    "hyperparams.update(hps)\n",
    "\n",
    "def objective(trial):\n",
    "    overrides = best_params(hyperparams)\n",
    "    base_model = trial.suggest_categorical(\"base_model\", [\"en_core_web_sm\", \"en_core_web_md\"])\n",
    "    for hp in hps:\n",
    "        overrides |= {hp: base_model}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_base_model\"):\n",
    "    study.optimize(objective, n_trials=2, callbacks=[archiver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = [\"paths.vectors\", \"components.ner.source\", \"components.tok2vec.source\", \"initialize.vectors\"]\n",
    "hyperparams.update(hps)\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    overrides = best_params(hyperparams)\n",
    "    static_vecs = trial.suggest_categorical(\"static_vecs\", [\"false\",\"true\"])\n",
    "    for hp in hps:\n",
    "        overrides |= {hp: \"en_core_web_md\"}\n",
    "    return objective_base(trial, overrides | {\"components.tok2vec.model.embed.include_static_vectors\": static_vecs})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_base_model_2\"):\n",
    "    study.optimize(objective, n_trials=2, callbacks=[archiver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = ['components.tok2vec.model.encode.width', 'components.tok2vec.model.encode.depth']\n",
    "hyperparams.update(hps)\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    overrides = best_params(hyperparams)\n",
    "    width = trial.suggest_categorical(\"tok2vec.width\", [96, 128, 160, 192, 224, 256])\n",
    "    depth = trial.suggest_categorical(\"tok2vec.depth\", [4, 8])\n",
    "    overrides |= {hps[0]: width, hps[1]: depth}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_tok2vec_size\"):\n",
    "    study.optimize(objective, n_trials=4, callbacks=[archiver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qjn)",
   "language": "python",
   "name": "qjn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
