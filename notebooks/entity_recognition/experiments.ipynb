{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from spacy.cli import apply\n",
    "from spacy.tokens import DocBin, Doc\n",
    "import pandas as pd\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline.functions import merge_entities\n",
    "from spacy.tokens import Span\n",
    "from spacy.util import filter_spans\n",
    "from spacy.matcher.phrasematcher import PhraseMatcher\n",
    "\n",
    "from scripts.utils.config import Config\n",
    "from scripts.geoms.operations import sides\n",
    "from scripts.utils.spacy import load_spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "train_path = config.get_data_path(\"entity_recognition.article_text_train\")\n",
    "dev_path = config.get_data_path(\"entity_recognition.article_text_dev\")\n",
    "explore_path = config.get_data_path(\"entity_recognition.explore\")\n",
    "sent_model = os.path.join(config.get_file_path(\"sent_relevance.trained_model\"), 'model-best')\n",
    "comm_area_path = config.get_data_path(\"geoms.comm_areas\")\n",
    "neighborhood_path = config.get_data_path(\"geoms.neighborhoods\")\n",
    "street_name_path = config.get_data_path(\"geoms.street_names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = load_spacy(\"en_core_web_sm\")\n",
    "nlp_base = load_spacy(\"en_core_web_sm\")\n",
    "\n",
    "# Reading this twice because both nlp's modifies the docs in place.\n",
    "docs_base = list(DocBin().from_disk(train_path).get_docs(nlp.vocab))\n",
    "docs_base = [d for d in docs_base if 'WHERE' in d.user_data and d.user_data['WHERE'] > .5]\n",
    "docs = list(DocBin().from_disk(train_path).get_docs(nlp.vocab))\n",
    "docs = [d for d in docs if 'WHERE' in d.user_data and d.user_data['WHERE'] > .5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpes = pd.concat([gpd.read_parquet(comm_area_path)['community_name'].rename('name'),\n",
    "                        pd.read_csv(neighborhood_path)['name'],\n",
    "                        pd.Series(sides)], ignore_index=True)\n",
    "gpes = gpes.str.split(\",\", expand=False).explode()\n",
    "gpes = gpes.str.title().drop_duplicates().sort_values()\n",
    "\n",
    "street_names = pd.read_csv(street_name_path)\n",
    "street_names = street_names.filter(like='combined').melt()['value']\n",
    "street_names = street_names.str.title().drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.pattern_matcher(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.tokenizer.pipe(gpes))\n",
    "matcher.add(\"GPE\", patterns)\n",
    "patterns = list(nlp.tokenizer.pipe(street_names))\n",
    "matcher.add(\"FAC\", patterns)\n",
    "\n",
    "@Language.component(\"loc_matcher\")\n",
    "def pattern_matcher(doc: Doc):\n",
    "    matches = matcher(doc, as_spans=True)\n",
    "    doc.ents = filter_spans(matches)\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"loc_matcher\", before=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.expand_street_blocks(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"block_matcher\")\n",
    "def expand_street_blocks(doc: Doc):\n",
    "    new_ents = []\n",
    "    for idx, ent in enumerate(doc.ents):\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"FAC\" and ent.start >= 3 and idx >= 1:\n",
    "            prev_ent = list(doc.ents)[idx-1]\n",
    "            prev_tokens = doc[ent.start - 3: ent.start]\n",
    "            # Must match [CARDINAL] block of [FAC]\n",
    "            if (prev_tokens[2].text == \"of\" and prev_tokens[1].text == \"block\"\n",
    "                and prev_ent.label_ == \"CARDINAL\" and prev_tokens[0].text == prev_ent.text):\n",
    "                new_ent = Span(doc, ent.start - 3, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "    doc.ents = filter_spans(list(doc.ents) + new_ents)\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"block_matcher\", after=\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.expand_intersections(doc: spacy.tokens.doc.Doc)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@Language.component(\"intersection_matcher\")\n",
    "def expand_intersections(doc: Doc):\n",
    "    new_ents = []\n",
    "    for idx, ent in enumerate(doc.ents):\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"FAC\" and ent.start >= 2 and idx >= 1:\n",
    "            prev_ent = list(doc.ents)[idx-1]\n",
    "            prev_tokens = doc[ent.start - 2: ent.start]\n",
    "            # Must match [STREET] and [STREET]\n",
    "            if ((prev_tokens[1].text == \"and\" or prev_tokens[1].text == \"&\")\n",
    "                and prev_ent.label_ == \"FAC\" and prev_tokens[0].text == prev_ent.text):\n",
    "                new_ent = Span(doc, ent.start - 2, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "    doc.ents = filter_spans(list(doc.ents) + new_ents)\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"intersection_matcher\", before=\"block_matcher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Twenty-three-year-old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Maurice Wiggins\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was sentenced \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Monday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    East St. Louis\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " on a felony charge of making a bomb threat.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# displacy.render(pred_base, style=\"ent\")\u001b[39;00m\n\u001b[32m     14\u001b[39m displacy.render(pred, style=\u001b[33m\"\u001b[39m\u001b[33ment\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m wait = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPress any key to continue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/quantify-news/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Dev/quantify-news/.venv/lib/python3.11/site-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def ents_eq(e1, e2):\n",
    "    return e1.text == e2.text and e1.start == e2.start \\\n",
    "        and e1.end == e2.end and e1.label == e2.label \\\n",
    "        and e1.label_ == e2.label_\n",
    "\n",
    "for doc_base, doc in zip(docs_base, docs):\n",
    "    # pred_base = nlp_base(doc_base)\n",
    "    pred = nlp(doc)\n",
    "    # if not all([ents_eq(e1, e2) for e1, e2 in zip(pred_base.ents, pred.ents)]):\n",
    "    clear_output(wait=True)\n",
    "        # displacy.render(pred_base, style=\"ent\")\n",
    "    displacy.render(pred, style=\"ent\")\n",
    "    wait = input(\"Press any key to continue\")\n",
    "    if wait:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qjn)",
   "language": "python",
   "name": "qjn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
