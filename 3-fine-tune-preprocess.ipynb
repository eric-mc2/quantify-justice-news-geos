{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "from spacy.training import Example\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "from thinc.api import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"gdrive/MyDrive/Work/quantify-news/\" if COLAB else \"data/\"\n",
    "PROJECT_DIR = \"gdrive/MyDrive/Work/quantify-news/\" if COLAB else \"./\"\n",
    "\n",
    "FULL_TEXT_TRAIN_PATH = DATA_DIR + \"newsarticles_article_train.parquet\"\n",
    "FULL_TEXT_DEV_PATH = DATA_DIR + \"newsarticles_article_dev.parquet\"\n",
    "FULL_TEXT_TEST_PATH = DATA_DIR + \"newsarticles_article_test.parquet\"\n",
    "\n",
    "USER_LABELS_TRAIN_PATH = DATA_DIR + \"newsarticles_usercoding_train.csv\"\n",
    "USER_LABELS_DEV_PATH = DATA_DIR + \"newsarticles_usercoding_dev.csv\"\n",
    "USER_LABELS_TEST_PATH = DATA_DIR + \"newsarticles_usercoding_test.csv\"\n",
    "\n",
    "DATA_TRAIN_BIN_PATH = DATA_DIR + \"ner_train.spacy\"\n",
    "DATA_DEV_BIN_PATH = DATA_DIR + \"ner_dev.spacy\"\n",
    "DATA_TEST_BIN_PATH = DATA_DIR + \"ner_test.spacy\"\n",
    "\n",
    "SPACY_CONFIG_PATH = PROJECT_DIR + (\"spacy_base_config_colab.cfg\" if COLAB else \"spacy_base_config.cfg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_data_train = pd.read_parquet(FULL_TEXT_TRAIN_PATH)\n",
    "article_data_dev = pd.read_parquet(FULL_TEXT_DEV_PATH)\n",
    "article_data_test = pd.read_parquet(FULL_TEXT_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data_train = pd.read_csv(USER_LABELS_TRAIN_PATH)\n",
    "loc_data_dev = pd.read_csv(USER_LABELS_DEV_PATH)\n",
    "loc_data_test = pd.read_csv(USER_LABELS_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_config = Config().from_disk(SPACY_CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = int(spacy_config['nlp']['batch_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Labels and Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(article_data, loc_data):\n",
    "    article_data['bodytext'] = (article_data['bodytext']\n",
    "                                  .str.replace('\\n',' ')\n",
    "                                  .str.replace(u'\\xa0', u' '))\n",
    "    \n",
    "    loc_data['loc_text'] = (loc_data['loc_text']\n",
    "                                  .str.replace('\\n',' ')\n",
    "                                  .str.replace(u'\\xa0', u' '))\n",
    "    \n",
    "    loc_texts = loc_data.groupby('article_id',as_index=False).agg({'loc_start':list, 'loc_end':list, 'loc_text':list})\n",
    "    ner_data = article_data.merge(loc_texts, left_on='id', right_on='article_id', how='inner')\n",
    "    return ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data_train = preproc(article_data_train, loc_data_train)\n",
    "ner_data_dev = preproc(article_data_dev, loc_data_dev)\n",
    "ner_data_test = preproc(article_data_test, loc_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del article_data_train\n",
    "del article_data_dev\n",
    "del article_data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch by length for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: I\"m not sure if this affects anything because spacy might randomize\n",
    "and re-shuffle the data anyways. And its batcher does group by similar-word-counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(ner_data):\n",
    "    q3 = ner_data.bodytext.str.len().quantile(.75)\n",
    "    q1 = ner_data.bodytext.str.len().quantile(.25)\n",
    "    iqr = q3 - q1\n",
    "    # left_outliers = ner_data.bodytext.str.len() < (q1 - 1.5 * iqr)\n",
    "    right_outliers = ner_data.bodytext.str.len() > (q3 + 1.5 * iqr)\n",
    "    # outliers = left_outliers | right_outliers\n",
    "    ner_data['very_long'] = right_outliers\n",
    "\n",
    "    df_batch_size = BATCH_SIZE * 4\n",
    "    ner_data_sorted = ner_data.sort_values('bodytext', key=lambda x: x.str.len())\n",
    "    return np.array_split(ner_data_sorted, len(ner_data) // df_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_train = batch_data(ner_data_train)\n",
    "batches_dev = batch_data(ner_data_dev)\n",
    "batches_test = batch_data(ner_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save batches to binary format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_blank = spacy.blank(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_data(batches, output):\n",
    "    doc_bin = DocBin(store_user_data=True)\n",
    "    for batch in tqdm(batches):\n",
    "        batch_size = 1 if batch['very_long'].any() else BATCH_SIZE\n",
    "        entities = batch.apply(lambda row: list(zip(row.loc_start, row.loc_end, ['NEWS_LOC']*len(row.loc_start))), axis=1)\n",
    "        docs = nlp_blank.pipe(batch.bodytext, batch_size=batch_size, disable=['ner'])\n",
    "        examples = [Example.from_dict(doc, {\"entities\": ent}) for doc, ent in zip(docs, entities)]\n",
    "        for eg in examples:\n",
    "            doc_bin.add(eg.reference)\n",
    "    doc_bin.to_disk(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binarize_data(batches_train, DATA_TRAIN_BIN_PATH)\n",
    "binarize_data(batches_dev, DATA_DEV_BIN_PATH)\n",
    "binarize_data(batches_test, DATA_TEST_BIN_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
