{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from git import Repo\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import secrets\n",
    "import spacy\n",
    "from spacy.util import load_config\n",
    "from spacy.cli import evaluate, apply\n",
    "from spacy.tokens import DocBin, Doc\n",
    "from spacy.scorer import Scorer\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "import optuna\n",
    "from thinc.api import Config as SpacyConfig\n",
    "\n",
    "import scripts.art_relevance.operations as ops\n",
    "from scripts.utils.config import Config\n",
    "from scripts.utils import load_spacy, flatten_config, nest_config\n",
    "from scripts.utils.spacy import load_metrics\n",
    "from scripts.utils.optuna import ArchiveBestModelCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"art_relevance_models\"\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"Prototype model architectures for article relevance classifier.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"qjn\",\n",
    "    \"task\": \"art_relevance\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.set_experiment_tags(experiment_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_param_keys(params: dict):\n",
    "    # XXX: This migth have a bug converting children to strings?\n",
    "    return {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_log_eval(run_name, params, model_path, nested=False):\n",
    "    \n",
    "    # Evaluate model\n",
    "    best_model_path = os.path.join(model_path, \"model-best\")\n",
    "    metrics = load_metrics(model_path)\n",
    "\n",
    "    # Load model params\n",
    "    repo = Repo(config._LOCAL_PROJECT_DIR, search_parent_directories=True)\n",
    "    params['git_hash'] = repo.heads.main.commit.hexsha\n",
    "\n",
    "    # Reshape params for logging\n",
    "    params = flatten_config(params)\n",
    "    # XXX: This migth have a bug converting children to strings?\n",
    "    params = {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}\n",
    "\n",
    "    # TODO: Maybe integrate with dagster? Maybe not? https://docs.dagster.io/api/python-api/libraries/dagster-mlflow\n",
    "    with mlflow.start_run(run_name=run_name, nested=nested) as run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.spacy.log_model(load_spacy(best_model_path), run_name)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "train_path = config.get_data_path(\"art_relevance.article_text_train\")\n",
    "dev_path = config.get_data_path(\"art_relevance.article_text_dev\")\n",
    "base_cfg = config.get_file_path(\"art_relevance.base_cfg\")\n",
    "full_cfg = config.get_file_path(\"art_relevance.full_cfg\")\n",
    "out_path = config.get_file_path(\"art_relevance.trained_model\")\n",
    "out_path_scratch = config.get_file_path(\"art_relevance.trained_model\", scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.init_config(base_cfg, full_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.train(train_path, dev_path, full_cfg, out_path)\n",
    "\n",
    "params = dict(load_config(full_cfg).interpolate())\n",
    "mlflow_log_eval(\"quickstart_model\", params, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_base(trial, overrides = {}):\n",
    "    ops.train(base_cfg, full_cfg, train_path, dev_path, out_path_scratch, overrides)\n",
    "    \n",
    "    # Train will keep base config and apply overrides at run-time.\n",
    "    # So we load the config with the overrides for logging.\n",
    "    params = dict(load_config(full_cfg, overrides).interpolate())\n",
    "\n",
    "    run_name = f\"optuna_trial_{trial.number}\"\n",
    "    metrics = mlflow_log_eval(run_name, params, out_path_scratch, nested=True)\n",
    "\n",
    "    return metrics['CRIME']['f']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=experiment_tags['task'],\n",
    "                            direction=\"maximize\",\n",
    "                            storage=config.get_param(\"art_relevance.optuna_db\"),\n",
    "                            load_if_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hp_start_size = trial.suggest_int(\"training.batcher.size.start\", 1, 100)  # Tune batch start\n",
    "    overrides = {\"training.batcher.size.start\": hp_start_size}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "archiver = ArchiveBestModelCallback(out_path=out_path, out_path_scratch=out_path_scratch)\n",
    "with mlflow.start_run(run_name=\"opt_batch_start_size\"):\n",
    "    study.optimize(objective, n_trials=10, callbacks=[archiver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    base_model = trial.suggest_categorical(\"paths.vectors\", [\"en_core_web_sm\", \"en_core_web_md\"])\n",
    "    overrides = {\"paths.vectors\": base_model,\n",
    "                 \"training.batcher.size.start\": study.best_params['training.batcher.size.start']}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_base_model\"):\n",
    "    study.optimize(objective, n_trials=2, callbacks=[archiver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: It looks like the hyperparameters aren't working, \n",
    "but we also only have 20 data points in the dev set so there aren't a lot of\n",
    "ways the predictions can fall. In the next section, I verify that a different\n",
    "hp set actually does return differering metrics per run. So the engineering is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial):\n",
    "    length = trial.suggest_int(\"components.textcat.model\", 1, 18)\n",
    "    overrides = {\"paths.vectors\": \"en_core_web_sm\",\n",
    "                 \"training.batcher.size.start\": 32,\n",
    "                 \"components.textcat.model.length\": 2**length}\n",
    "    # load_config(base_cfg, overrides).to_disk(base_cfg)\n",
    "    # ops.init_config(base_cfg, full_cfg)\n",
    "    return objective_base(trial, overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"opt_linear_length\"):\n",
    "    study.optimize(objective, n_trials=9, callbacks=[archiver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run null_model_expectation at: http://127.0.0.1:8080/#/experiments/937901472817136779/runs/652fdf563458421081a1315873a04d67\n",
      "üß™ View experiment at: http://127.0.0.1:8080/#/experiments/937901472817136779\n"
     ]
    }
   ],
   "source": [
    "class NullModel:\n",
    "    pos_percent = 0\n",
    "\n",
    "    def train(self, data_path: str):\n",
    "        blank = spacy.blank(\"en\")\n",
    "        docs = DocBin().from_disk(data_path).get_docs(blank.vocab)\n",
    "        self.pos_percent = np.mean([d.cats['CRIME']==1 for d in docs])\n",
    "\n",
    "    def eval(self, data_path):\n",
    "        blank = spacy.blank(\"en\")\n",
    "        docs = list(DocBin().from_disk(data_path).get_docs(blank.vocab))\n",
    "        trials = []\n",
    "        for _ in range(100):\n",
    "            rng = np.random.default_rng(seed=secrets.randbits(128))\n",
    "                \n",
    "            preds = np.where(rng.random((len(docs),)) < self.pos_percent, \n",
    "                            {'CRIME': 1, 'IRRELEVANT': 0}, \n",
    "                            {'IRRELEVANT': 1, 'CRIME': 0})\n",
    "            tp = sum([d.cats['CRIME']==1 and p['CRIME']==1 for d,p in zip(docs, preds)])\n",
    "            fp = sum([d.cats['CRIME']==0 and p['CRIME']==1 for d,p in zip(docs, preds)])\n",
    "            fn = sum([d.cats['CRIME']==1 and p['CRIME']==0 for d,p in zip(docs, preds)])\n",
    "            trials.append(dict(\n",
    "                precision = tp / (tp + fp),\n",
    "                recall = tp / (tp + fn),\n",
    "                f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "            ))\n",
    "        return pd.DataFrame.from_records(trials).mean().to_dict()\n",
    "    \n",
    "null_model = NullModel()\n",
    "null_model.train(train_path)\n",
    "metrics = null_model.eval(dev_path)\n",
    "with mlflow.start_run(run_name=\"null_model_expectation\", nested=False) as run:\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.log_param(\"pos_proba\", null_model.pos_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_run = mlflow.search_runs(\n",
    "        experiment_names=[experiment_name],\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=1,\n",
    "        order_by=[\"metrics.f1 DESC\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't needed anymore because I found where mlflow saves the actual spacy models.\n",
    "# But might be nice to have later. \n",
    "def mlflow_to_cfg(best_runs: pd.DataFrame) -> SpacyConfig:\n",
    "    best_params = best_runs.iloc[0][best_runs.iloc[0].index.str.startswith('params')]\n",
    "    best_params = best_params.replace(\"None\",\"null\")\n",
    "    # XXX: This migth have a bug converting children to strings?\n",
    "    best_params = {k.replace(\"_AT_\",\"@\"):v for k,v in best_params.to_dict().items()}\n",
    "    best_params = nest_config(best_params)['params']\n",
    "    del best_params['git_hash']\n",
    "    best_config = SpacyConfig(best_params)\n",
    "    best_config.to_disk(full_cfg)\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_uri = os.path.join(highest_accuracy_run.iloc[0]['artifact_uri'],\"optuna_trial_25\", \"model.spacy\")\n",
    "best_metrics = evaluate(\n",
    "    best_model_uri,\n",
    "    dev_path,\n",
    ")\n",
    "best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(data_path=Path(dev_path), \n",
    "      output_file=Path(\"./preds.spacy\"), \n",
    "      model=best_model_uri, \n",
    "      json_field=\"text\", \n",
    "      batch_size=1,\n",
    "      n_process=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_pred = DocBin().from_disk(\"./preds.spacy\")\n",
    "docs_gold = DocBin().from_disk(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for pred,gold in zip(docs_pred.get_docs(spacy.blank(\"en\").vocab),\n",
    "                     docs_gold.get_docs(spacy.blank(\"en\").vocab)):\n",
    "    assert pred.text == gold.text\n",
    "    records.append({'text': gold.text, \n",
    "                    'label': 'CRIME' if gold.cats['CRIME'] > gold.cats['IRRELEVANT'] else 'IRRELEVANT',\n",
    "                    'IRRELEVANT': pred.cats['IRRELEVANT'],\n",
    "                    'CRIME': pred.cats['CRIME']})\n",
    "preds = pd.DataFrame.from_records(records)\n",
    "preds['tp'] = (preds['CRIME'] > preds['IRRELEVANT']) & (preds['label'] == 'CRIME')\n",
    "preds['fp'] = (preds['CRIME'] > preds['IRRELEVANT']) & (preds['label'] != 'CRIME')\n",
    "preds['tn'] = (preds['CRIME'] <= preds['IRRELEVANT']) & (preds['label'] != 'CRIME')\n",
    "preds['fn'] = (preds['CRIME'] <= preds['IRRELEVANT']) & (preds['label'] == 'CRIME')\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "At this point i'm guessing hp tuning won't help the model.\n",
    "It would be better to either label more data and try again,\n",
    "or move onto the next part, maybe re-optimizing in the interim for\n",
    "precision to reduce the amount of sentences labels I have to manually throw out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qjn)",
   "language": "python",
   "name": "qjn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
