{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "import mlflow\n",
    "from mlflow.entities import ViewType\n",
    "from spacy.util import load_config\n",
    "import json\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import shutil\n",
    "from thinc.api import Config as SpacyConfig\n",
    "from spacy.cli import evaluate, apply\n",
    "from pathlib import Path\n",
    "\n",
    "import scripts.art_relevance.operations as ops\n",
    "from scripts.utils.config import Config\n",
    "from scripts.utils import load_spacy, flatten_config, nest_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"art_relevance_models\"\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"Prototype model architectures for article relevance classifier.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"qjn\",\n",
    "    \"task\": \"art_relevance\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.set_experiment_tags(experiment_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_param_keys(params: dict):\n",
    "    return {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_log_eval(run_name, params, model_path, nested=False):\n",
    "    \n",
    "    # Evaluate model\n",
    "    best_model_path = os.path.join(model_path, \"model-best\")\n",
    "    metric_file = os.path.join(best_model_path, \"meta.json\")\n",
    "    with open(metric_file) as fp:\n",
    "        metrics = json.load(fp)['performance']\n",
    "    track_metrics = dict(\n",
    "        precision = metrics['cats_f_per_type']['CRIME']['p'],\n",
    "        recall = metrics['cats_f_per_type']['CRIME']['r'],\n",
    "        f1 = metrics['cats_f_per_type']['CRIME']['f'],\n",
    "    )\n",
    "\n",
    "    # Load model params\n",
    "    repo = Repo(config._LOCAL_PROJECT_DIR, search_parent_directories=True)\n",
    "    params['git_hash'] = repo.heads.main.commit.hexsha\n",
    "\n",
    "    # Reshape params for logging\n",
    "    params = flatten_config(params)\n",
    "    params = {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}\n",
    "\n",
    "    # TODO: Maybe integrate with dagster? Maybe not? https://docs.dagster.io/api/python-api/libraries/dagster-mlflow\n",
    "    with mlflow.start_run(run_name=run_name, nested=nested) as run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(track_metrics)\n",
    "        mlflow.spacy.log_model(load_spacy(best_model_path), run_name)\n",
    "\n",
    "    return track_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "train_path = config.get_data_path(\"art_relevance.article_text_train\")\n",
    "dev_path = config.get_data_path(\"art_relevance.article_text_dev\")\n",
    "base_cfg = config.get_file_path(\"art_relevance.base_cfg\")\n",
    "full_cfg = config.get_file_path(\"art_relevance.full_cfg\")\n",
    "out_path = config.get_file_path(\"art_relevance.trained_model\")\n",
    "out_path_scratch = config.get_file_path(\"art_relevance.trained_model\", scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.init_config(base_cfg, full_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.train(train_path, dev_path, full_cfg, out_path)\n",
    "\n",
    "params = dict(load_config(full_cfg).interpolate())\n",
    "mlflow_log_eval(\"quickstart_model\", params, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_base(trial, overrides = {}):\n",
    "    ops.train(train_path, dev_path, full_cfg, out_path_scratch, overrides)\n",
    "    \n",
    "    # Train will keep base config and apply overrides at run-time.\n",
    "    # So we load the config with the overrides for logging.\n",
    "    params = dict(load_config(full_cfg, overrides).interpolate())\n",
    "\n",
    "    run_name = f\"optuna_trial_{trial.number}\"\n",
    "    metrics = mlflow_log_eval(run_name, params, out_path_scratch, nested=True)\n",
    "\n",
    "    return metrics['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchiveBestModelCallback:\n",
    "    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
    "        best_scratch = os.path.join(out_path_scratch,'model-best')\n",
    "        best_stable = os.path.join(out_path,'model-best')\n",
    "        if best_stable == best_scratch:\n",
    "            return # Nothing to do.\n",
    "        if ((study.direction == optuna.study.StudyDirection.MAXIMIZE\n",
    "            and study.best_value <= trial.value) or\n",
    "            (study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "             and study.best_value >= trial.value)):\n",
    "            shutil.copytree(best_scratch, best_stable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=experiment_tags['task'],\n",
    "                            direction=\"maximize\",\n",
    "                            storage=config.get_param(\"art_relevance.optuna_db\"),\n",
    "                            load_if_exists=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hp_start_size = trial.suggest_int(\"training.batcher.size.start\", 1, 100)  # Tune batch start\n",
    "    overrides = {\"training.batcher.size.start\": hp_start_size}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_batch_start_size\"):\n",
    "    study.optimize(objective, n_trials=10, callbacks=[ArchiveBestModelCallback()])\n",
    "    mlflow.log_params({\n",
    "        f\"best_{k}\": v for k, v in study.best_params.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    base_model = trial.suggest_categorical(\"paths.vectors\", [\"en_core_web_sm\", \"en_core_web_md\"])\n",
    "    overrides = {\"paths.vectors\": base_model,\n",
    "                 \"training.batcher.size.start\": study.best_params['training.batcher.size.start']}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_base_model\"):\n",
    "    study.optimize(objective, n_trials=2, callbacks=[ArchiveBestModelCallback()])\n",
    "    mlflow.log_params({\n",
    "        f\"best_{k}\": v for k, v in study.best_params.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: It looks like the hyperparameters aren't working, \n",
    "but we also only have 20 data points in the dev set so there aren't a lot of\n",
    "ways the predictions can fall. In the next section, I verify that a different\n",
    "hp set actually does return differering metrics per run. So the engineering is working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow Length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial: optuna.trial.Trial):\n",
    "    length = trial.suggest_int(\"components.textcat.model\", 1, 18)\n",
    "    overrides = {\"paths.vectors\": \"en_core_web_sm\",\n",
    "                 \"training.batcher.size.start\": 32,\n",
    "                 \"components.textcat.model.length\": 2**length}\n",
    "    # load_config(base_cfg, overrides).to_disk(base_cfg)\n",
    "    # ops.init_config(base_cfg, full_cfg)\n",
    "    return objective_base(trial, overrides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"opt_linear_length\"):\n",
    "    study.optimize(objective, n_trials=9, callbacks=[ArchiveBestModelCallback()])\n",
    "    mlflow.log_params({\n",
    "        f\"best_{k}\": v for k, v in study.best_params.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_accuracy_run = mlflow.search_runs(\n",
    "        experiment_names=[experiment_name],\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=1,\n",
    "        order_by=[\"metrics.f1 DESC\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This isn't needed anymore because I found where mlflow saves the actual spacy models.\n",
    "# But might be nice to have later. \n",
    "def mlflow_to_cfg(best_runs: pd.DataFrame) -> SpacyConfig:\n",
    "    best_params = best_runs.iloc[0][best_runs.iloc[0].index.str.startswith('params')]\n",
    "    best_params = best_params.replace(\"None\",\"null\")\n",
    "    best_params = {k.replace(\"_AT_\",\"@\"):v for k,v in best_params.to_dict().items()}\n",
    "    best_params = nest_config(best_params)['params']\n",
    "    del best_params['git_hash']\n",
    "    best_config = SpacyConfig(best_params)\n",
    "    best_config.to_disk(full_cfg)\n",
    "    return best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_uri = os.path.join(highest_accuracy_run.iloc[0]['artifact_uri'],\"optuna_trial_25\", \"model.spacy\")\n",
    "best_metrics = evaluate(\n",
    "    best_model_uri,\n",
    "    dev_path,\n",
    ")\n",
    "best_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(data_path=Path(dev_path), \n",
    "      output_file=Path(\"./preds.spacy\"), \n",
    "      model=best_model_uri, \n",
    "      json_field=\"text\", \n",
    "      batch_size=1,\n",
    "      n_process=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_pred = DocBin().from_disk(\"./preds.spacy\")\n",
    "docs_gold = DocBin().from_disk(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for pred,gold in zip(docs_pred.get_docs(spacy.blank(\"en\").vocab),\n",
    "                     docs_gold.get_docs(spacy.blank(\"en\").vocab)):\n",
    "    assert pred.text == gold.text\n",
    "    records.append({'text': gold.text, \n",
    "                    'label': 'CRIME' if gold.cats['CRIME'] > gold.cats['IRRELEVANT'] else 'IRRELEVANT',\n",
    "                    'IRRELEVANT': pred.cats['IRRELEVANT'],\n",
    "                    'CRIME': pred.cats['CRIME']})\n",
    "preds = pd.DataFrame.from_records(records)\n",
    "preds['tp'] = (preds['CRIME'] > preds['IRRELEVANT']) & (preds['label'] == 'CRIME')\n",
    "preds['fp'] = (preds['CRIME'] > preds['IRRELEVANT']) & (preds['label'] != 'CRIME')\n",
    "preds['tn'] = (preds['CRIME'] <= preds['IRRELEVANT']) & (preds['label'] != 'CRIME')\n",
    "preds['fn'] = (preds['CRIME'] <= preds['IRRELEVANT']) & (preds['label'] == 'CRIME')\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "At this point i'm guessing hp tuning won't help the model.\n",
    "It would be better to either label more data and try again,\n",
    "or move onto the next part, maybe re-optimizing in the interim for\n",
    "precision to reduce the amount of sentences labels I have to manually throw out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qjn)",
   "language": "python",
   "name": "qjn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
