{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "import mlflow\n",
    "from spacy.util import load_config\n",
    "import json\n",
    "import optuna\n",
    "import shutil\n",
    "\n",
    "import scripts.art_relevance.operations as ops\n",
    "from scripts.utils.config import Config\n",
    "from scripts.utils import load_spacy, flatten_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup MLFLow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "    \"Prototype model architectures for article relevance classifier.\"\n",
    ")\n",
    "\n",
    "# Provide searchable tags that define characteristics of the Runs that\n",
    "# will be in this Experiment\n",
    "experiment_tags = {\n",
    "    \"project_name\": \"qjn\",\n",
    "    \"task\": \"art_relevance\",\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"art_relevance_models\")\n",
    "mlflow.set_experiment_tags(experiment_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_param_keys(params: dict):\n",
    "    return {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlflow_log_eval(run_name, params, model_path, nested=False):\n",
    "    \n",
    "    # Evaluate model\n",
    "    best_model_path = os.path.join(model_path, \"model-best\")\n",
    "    metric_file = os.path.join(best_model_path, \"meta.json\")\n",
    "    with open(metric_file) as fp:\n",
    "        metrics = json.load(fp)['performance']\n",
    "    track_metrics = dict(\n",
    "        precision = metrics['cats_f_per_type']['CRIME']['p'],\n",
    "        recall = metrics['cats_f_per_type']['CRIME']['r'],\n",
    "        f1 = metrics['cats_f_per_type']['CRIME']['f'],\n",
    "    )\n",
    "\n",
    "    # Load model params\n",
    "    repo = Repo(config._LOCAL_PROJECT_DIR, search_parent_directories=True)\n",
    "    params['git_hash'] = repo.heads.main.commit.hexsha\n",
    "\n",
    "    # Reshape params for logging\n",
    "    params = flatten_config(params)\n",
    "    params = {k.replace(\"@\",\"_AT_\"): v for k,v in params.items()}\n",
    "\n",
    "    # TODO: Maybe integrate with dagster? Maybe not? https://docs.dagster.io/api/python-api/libraries/dagster-mlflow\n",
    "    with mlflow.start_run(run_name=run_name, nested=nested) as run:\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metrics(track_metrics)\n",
    "        mlflow.spacy.log_model(load_spacy(best_model_path), run_name)\n",
    "\n",
    "    return track_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "train_path = config.get_data_path(\"art_relevance.article_text_train\")\n",
    "dev_path = config.get_data_path(\"art_relevance.article_text_dev\")\n",
    "base_cfg = config.get_file_path(\"art_relevance.base_cfg\")\n",
    "full_cfg = config.get_file_path(\"art_relevance.full_cfg\")\n",
    "out_path = config.get_file_path(\"art_relevance.trained_model\")\n",
    "out_path_scratch = config.get_file_path(\"art_relevance.trained_model\", scratch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.init_config(base_cfg, full_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops.train(train_path, dev_path, full_cfg, out_path)\n",
    "\n",
    "params = dict(load_config(full_cfg).interpolate())\n",
    "mlflow_log_eval(\"quickstart_model\", params, out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_base(trial, overrides):\n",
    "    ops.train(train_path, dev_path, full_cfg, out_path_scratch, overrides)\n",
    "    \n",
    "    # Train will keep base config and apply overrides at run-time.\n",
    "    # So we load the config with the overrides for logging.\n",
    "    params = dict(load_config(full_cfg, overrides).interpolate())\n",
    "\n",
    "    run_name = f\"optuna_trial_{trial.number}\"\n",
    "    metrics = mlflow_log_eval(run_name, params, out_path_scratch, nested=True)\n",
    "\n",
    "    return metrics['f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArchiveBestModelCallback:\n",
    "    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n",
    "        best_scratch = os.path.join(out_path_scratch,'model-best')\n",
    "        best_stable = os.path.join(out_path,'model-best')\n",
    "        if best_stable == best_scratch:\n",
    "            return # Nothing to do.\n",
    "        if ((study.direction == optuna.study.StudyDirection.MAXIMIZE\n",
    "            and study.best_value <= trial.value) or\n",
    "            (study.direction == optuna.study.StudyDirection.MINIMIZE\n",
    "             and study.best_value >= trial.value)):\n",
    "            shutil.copytree(best_scratch, best_stable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    hp_start_size = trial.suggest_int(\"training.batcher.size.start\", 1, 100)  # Tune batch start\n",
    "    overrides = {\"training.batcher.size.start\": hp_start_size}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "study = optuna.create_study(study_name=experiment_tags['task'],\n",
    "                            direction=\"maximize\",\n",
    "                            storage=config.get_param(\"art_relevance.optuna_db\"),\n",
    "                            load_if_exists=True)\n",
    "with mlflow.start_run(run_name=\"opt_batch_start_size\"):\n",
    "    study.optimize(objective, n_trials=10, callbacks=[ArchiveBestModelCallback()])\n",
    "    mlflow.log_params({\n",
    "        f\"best_{k}\": v for k, v in study.best_params.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    base_model = trial.suggest_categorical(\"paths.vectors\", [\"en_core_web_sm\", \"en_core_web_md\"])\n",
    "    overrides = {\"paths.vectors\": base_model,\n",
    "                 \"training.batcher.size.start\": study.best_params['training.batcher.size.start']}\n",
    "    return objective_base(trial, overrides)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: as currently configured, the optuna_db goes into the caller of create_study ie the notebook\n",
    "with mlflow.start_run(run_name=\"opt_base_model\"):\n",
    "    study.optimize(objective, n_trials=2, callbacks=[ArchiveBestModelCallback()])\n",
    "    mlflow.log_params({\n",
    "        f\"best_{k}\": v for k, v in study.best_params.items()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (qjn)",
   "language": "python",
   "name": "qjn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
